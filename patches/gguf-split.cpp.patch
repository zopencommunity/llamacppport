diff --git a/tools/gguf-split/gguf-split.cpp b/tools/gguf-split/gguf-split.cpp
index 30e77156..0472e387 100644
--- a/tools/gguf-split/gguf-split.cpp
+++ b/tools/gguf-split/gguf-split.cpp
@@ -328,14 +328,20 @@ struct split_strategy {
                 const char * t_name = gguf_get_tensor_name(ctx_out, i);
                 struct ggml_tensor * t = ggml_get_tensor(ctx_meta, t_name);
                 auto n_bytes = ggml_nbytes(t);
-                read_buf.resize(n_bytes);
+                auto n_elements = ggml_nelements(t) / ggml_blck_size(t->type);
+		read_buf.resize(n_bytes);
 
                 // calculate offset
                 auto i_tensor_in = gguf_find_tensor(ctx_gguf, t_name); // idx of tensor in the input file
                 auto offset = gguf_get_data_offset(ctx_gguf) + gguf_get_tensor_offset(ctx_gguf, i_tensor_in);
 
+		ggml_byteswap_t byteswap_func = nullptr;
+		if (gguf_needs_byteswap(ctx_gguf)) {
+		    byteswap_func = ggml_get_type_traits(t->type)->byteswap;
+		}
+
                 // copy tensor from input to output file
-                copy_file_to_file(f_input, fout, offset, n_bytes);
+                copy_file_to_file(f_input, fout, offset, n_bytes, n_elements, byteswap_func);
                 zeros(fout, GGML_PAD(n_bytes, GGUF_DEFAULT_ALIGNMENT) - n_bytes);
             }
 
@@ -346,13 +352,18 @@ struct split_strategy {
         }
     }
 
-    void copy_file_to_file(std::ifstream & f_in, std::ofstream & f_out, const size_t in_offset, const size_t len) {
+    void copy_file_to_file(std::ifstream & f_in, std::ofstream & f_out, const size_t in_offset, const size_t len, const size_t elements, ggml_byteswap_t byteswap_func) {
         // TODO: detect OS and use copy_file_range() here for better performance
         if (read_buf.size() < len) {
             read_buf.resize(len);
         }
         f_in.seekg(in_offset);
         f_in.read((char *)read_buf.data(), len);
+
+	if (byteswap_func != nullptr) {
+	    byteswap_func(read_buf.data(), elements);
+	}
+
         f_out.write((const char *)read_buf.data(), len);
     }
 };
@@ -363,6 +374,7 @@ static void gguf_split(const split_params & split_params) {
     struct gguf_init_params params = {
         /*.no_alloc = */ true,
         /*.ctx      = */ &ctx_meta,
+	/*.allow_byteswapping = */ true,
     };
 
     std::ifstream f_input(split_params.input.c_str(), std::ios::binary);
@@ -426,6 +438,7 @@ static void gguf_merge(const split_params & split_params) {
         struct gguf_init_params params = {
             /*.no_alloc = */ true,
             /*.ctx      = */ &ctx_meta,
+	    /*.allow_byteswapping = */ true,
         };
 
         if (i_split > 0) {
@@ -539,7 +552,15 @@ static void gguf_merge(const split_params & split_params) {
             auto offset = gguf_get_data_offset(ctx_gguf) + gguf_get_tensor_offset(ctx_gguf, i_tensor);
             f_input.seekg(offset);
             f_input.read((char *)read_data.data(), n_bytes);
-            if (!split_params.dry_run) {
+            
+	    if (gguf_needs_byteswap(ctx_gguf)) {
+		auto byteswap = ggml_get_type_traits(t->type)->byteswap;
+		if (byteswap != nullptr) {
+		    byteswap(read_data.data(), ggml_nelements(t) / ggml_blck_size(t->type));
+		}
+	    }
+	    
+	    if (!split_params.dry_run) {
                 // write tensor data + padding
                 fout.write((const char *)read_data.data(), n_bytes);
                 zeros(fout, GGML_PAD(n_bytes, GGUF_DEFAULT_ALIGNMENT) - n_bytes);
